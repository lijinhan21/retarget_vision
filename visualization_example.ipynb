{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from orion.algos.human_video_oogs import HumanVideoOOGs\n",
    "from orion.algos.oog import OpenWorldObjectSceneGraph\n",
    "from orion.utils.misc_utils import *\n",
    "from orion.utils.o3d_utils import *\n",
    "human_video_oogs = HumanVideoOOGs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "human_video_annotation_path = \"annotations/human_demo/iphone_place_chip_bag_on_plate_1_demo\" # iphone_front_boat_demo\n",
    "human_video_oogs.init()\n",
    "human_video_oogs.generate_from_human_video(human_video_annotation_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 3\n",
    "\n",
    "human_video_oogs.get_graph(0).draw_overlay_image(mode=\"object\")\n",
    "\n",
    "input_annotation = human_video_oogs.get_graph(idx).input_annotation\n",
    "overlay_image = overlay_xmem_mask_on_image(\n",
    "                human_video_oogs.get_graph(idx).input_image, \n",
    "                human_video_oogs.get_graph(idx).input_annotation,\n",
    "                use_white_bg=False,\n",
    "                rgb_alpha=0.3)\n",
    "\n",
    "\n",
    "\n",
    "overlay_image = overlay_image * (input_annotation == 0)[..., None] + human_video_oogs.get_graph(idx).input_image * (input_annotation != 0)[..., None]\n",
    "plotly_draw_image(overlay_image, width=640, height=480)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "human_video_annotation_path = \"annotations/human_demo/iphone_front_boat_demo\" # iphone_front_boat_demo\n",
    "human_video_oogs.init()\n",
    "human_video_oogs.generate_from_human_video(human_video_annotation_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Source image and depth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotly_draw_image(human_video_oogs.get_graph(2).input_image, width=640, height=480)\n",
    "plotly_draw_image(human_video_oogs.get_graph(1).input_depth, width=640, height=480)\n",
    "human_video_oogs.get_graph(0).draw_overlay_image(mode=\"point\", width=640, height=480, default_point_color=\"rgb(30, 122, 122)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Draw the scene and the objects from human video\n",
    "1. Draw the scene\n",
    "2. Draw the scene with objects being annotated with segmentation mask\n",
    "3. Trajectories that show the 3D motion of the objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "human_video_oogs.get_graph(1).draw_scene_3d(downsample=False, \n",
    "                                            depth_trunc=1.0,  \n",
    "                                            use_white_bg=True,\n",
    "                                            no_background=True, \n",
    "                                            overlay_annotation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trajs = human_video_oogs.get_graph(1).get_world_trajs(object_ids=[2])\n",
    "trajs = np.delete(trajs, 34, axis=0)\n",
    "human_video_oogs.get_graph(1).draw_objects_list_3d(object_ids=[1, 2], downsample=False, no_background=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trajs = human_video_oogs.get_graph(1).get_world_trajs(object_ids=[2])\n",
    "trajs = np.delete(trajs, [10, 17], axis=0)\n",
    "human_video_oogs.get_graph(1).draw_scene_3d(\n",
    "                                            additional_points=trajs,\n",
    "                                            downsample=False, no_background=True,\n",
    "                                            depth_trunc=1.0,  overlay_annotation=True, \n",
    "                                            additional_point_draw_lines=True,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Draw individual object point clouds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "human_video_oogs.get_graph(1).draw_objects_3d(object_id=2, downsample=False,  no_background=True, draw_keypoints=True, keypoints_rgb_str=\"(30, 200, 200)\", uniform_color=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trajs = human_video_oogs.get_graph(1).get_world_trajs(object_ids=[1])\n",
    "trajs = np.delete(trajs, [10, 17], axis=0)\n",
    "human_video_oogs.get_graph(1).draw_objects_3d(object_id=1, downsample=False,  no_background=True, draw_trajs=False, additional_point_draw_lines=True, keypoints_rgb_str=\"(30, 122, 122)\", \n",
    "                                              additional_points=trajs, \n",
    "                                              uniform_color=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import trimesh\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "# Load the .obj file using trimesh\n",
    "mesh = trimesh.load('annotations/human_demo/iphone_front_boat_demo/hamer_output/0039_0.obj')\n",
    "\n",
    "# Extract vertices and faces\n",
    "vertices = mesh.vertices\n",
    "faces = mesh.faces\n",
    "\n",
    "# Create a 3D plot\n",
    "fig = go.Figure(data=[go.Mesh3d(x=vertices[:, 0], y=vertices[:, 1], z=vertices[:, 2], \n",
    "                                i=faces[:, 0], j=faces[:, 1], k=faces[:, 2], \n",
    "                                 opacity=0.50)])\n",
    "\n",
    "# Adjust the camera view as desired\n",
    "fig.update_layout(scene_camera=dict(\n",
    "    eye=dict(x=1.5, y=1.5, z=1.5)  # Example camera position\n",
    "),\n",
    "        scene=dict(\n",
    "            xaxis=dict(showbackground=False, zeroline=False, showgrid=False, showticklabels=False, showaxeslabels=False, visible=False),\n",
    "            yaxis=dict(showbackground=False, zeroline=False, showgrid=False, showticklabels=False, showaxeslabels=False, visible=False),\n",
    "            zaxis=dict(showbackground=False, zeroline=False, showgrid=False, showticklabels=False, showaxeslabels=False, visible=False),\n",
    "        ),\n",
    "        paper_bgcolor='rgba(0,0,0,0)',  # Transparent background\n",
    "        plot_bgcolor='rgba(0,0,0,0)',  # Transparent background\n",
    "        margin=dict(l=0, r=0, b=0, t=0),  # No margins\n",
    "        showlegend=False,\n",
    ")\n",
    "\n",
    "\n",
    "fig.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dinov2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
