{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This is a script to verify the camera observations of the real robot. We assume that you are using deoxys_vision for capturing images. If you are using a different vision pipeline, please modify the code accordingly. \n",
    "\"\"\"\n",
    "import argparse\n",
    "import os\n",
    "import cv2\n",
    "import plotly.graph_objs as go\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from easydict import EasyDict\n",
    "from deoxys import config_root\n",
    "from deoxys.franka_interface import FrankaInterface\n",
    "from deoxys.utils import YamlConfig\n",
    "from deoxys.utils.input_utils import input2action\n",
    "from deoxys.utils.io_devices import SpaceMouse\n",
    "from deoxys.utils.log_utils import get_deoxys_example_logger\n",
    "\n",
    "from deoxys_vision.networking.camera_redis_interface import \\\n",
    "    CameraRedisSubInterface\n",
    "from deoxys_vision.utils.calibration_utils import load_default_extrinsics, load_default_intrinsics\n",
    "from deoxys_vision.utils.camera_utils import assert_camera_ref_convention, get_camera_info\n",
    "from deoxys_vision.utils.img_utils import save_depth_in_rgb\n",
    "\n",
    "from orion.algos.grounded_sam_wrapper import GroundedSamWrapper\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from orion.utils.misc_utils import depth_to_rgb, save_depth\n",
    "from orion.utils.o3d_utils import O3DPointCloud\n",
    "\n",
    "from orion.utils.real_robot_utils import RealRobotObsProcessor\n",
    "\n",
    "wrapper = GroundedSamWrapper()\n",
    "\n",
    "def capture_image():\n",
    "    # Make sure that you've launched camera nodes somewhere else\n",
    "    observation_cfg = YamlConfig(\"./configs/real_robot_observation_cfg.yml\").as_easydict()\n",
    "\n",
    "    observation_cfg.cameras = []\n",
    "    for camera_ref in observation_cfg.camera_refs:\n",
    "        assert_camera_ref_convention(camera_ref)\n",
    "        camera_info = get_camera_info(camera_ref)\n",
    "\n",
    "        observation_cfg.cameras.append(camera_info)\n",
    "\n",
    "    obs_processor = RealRobotObsProcessor(observation_cfg,\n",
    "                                            processor_name=\"ImageProcessor\")\n",
    "    obs_processor.load_intrinsic_matrix(resize=False)\n",
    "    obs_processor.load_extrinsic_matrix()\n",
    "    extrinsic_matrix = obs_processor.get_extrinsic_matrix(\"agentview\")\n",
    "    intrinsic_matrix = obs_processor.get_intrinsic_matrix(\"agentview\")\n",
    "\n",
    "    color_imgs, depth_imgs = obs_processor.get_original_imgs()\n",
    "    return color_imgs[0], depth_imgs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import yaml\n",
    "\n",
    "from orion.utils.misc_utils import get_palette\n",
    "\n",
    "def load_from_runtime_config():\n",
    "    with open(\"experiments/run_time.json\") as f:\n",
    "        run_time_config = json.load(f)\n",
    "    return run_time_config\n",
    "\n",
    "\n",
    "with open(\"experiments/experiment_cfg.yaml\", \"r\") as f:\n",
    "    cfg = yaml.load(f, Loader=yaml.FullLoader)\n",
    "task_mapping = cfg[\"task_mapping\"]\n",
    "task_id_mapping = cfg[\"task_id_mapping\"]\n",
    "model_mapping = cfg[\"model_mapping\"]\n",
    "demo_mapping = cfg[\"demo_mapping\"]\n",
    "result_mapping = cfg[\"result_mapping\"]\n",
    "\n",
    "runtime_config = load_from_runtime_config()\n",
    "# print(runtime_config)\n",
    "runtime_folder = runtime_config[\"runtime_folder\"]\n",
    "\n",
    "task_id = runtime_folder.split(\"/\")[-3]\n",
    "model_id = runtime_folder.split(\"/\")[-2]\n",
    "# captialize all letters in model_id\n",
    "model_id = model_id.upper()\n",
    "run_id = runtime_folder.split(\"/\")[-1]\n",
    "\n",
    "task_name_str = task_id_mapping[task_id]\n",
    "\n",
    "rollout_name = f\"{task_name_str}_{model_id}_{run_id}\"\n",
    "\n",
    "rollout_prefix = \"annotations/rollout\"\n",
    "rollout_folder = f\"{rollout_prefix}/{rollout_name}\"\n",
    "\n",
    "human_video_annotation_folder = f\"annotations/human_demo/{demo_mapping[task_id]}\"\n",
    "os.path.exists(human_video_annotation_folder)\n",
    "\n",
    "with open(os.path.join(human_video_annotation_folder, \"config.json\")) as f:\n",
    "    config_info = json.load(f)\n",
    "\n",
    "args = EasyDict({\n",
    "    \"rollout_name\": rollout_name,\n",
    "    \"text\": config_info[\"text\"]\n",
    "})\n",
    "\n",
    "print(f\"Annotations to {rollout_name} with text: {args.text}\")\n",
    "\n",
    "args = EasyDict(args)\n",
    "color_img, depth_img = capture_image()\n",
    "\n",
    "annotation_path = os.path.join(\"annotations\", \"rollout\", args.rollout_name)\n",
    "os.makedirs(annotation_path, exist_ok=True)\n",
    "\n",
    "captured_rgb_name = os.path.join(annotation_path, f\"{args.rollout_name}_rgb.jpg\")\n",
    "captured_depth_name = os.path.join(annotation_path, f\"depth.png\")\n",
    "frame_path = os.path.join(annotation_path, f\"frame.jpg\")\n",
    "\n",
    "cv2.imwrite(captured_rgb_name, color_img)\n",
    "cv2.imwrite(frame_path, color_img)\n",
    "save_depth_in_rgb(captured_depth_name, depth_img)\n",
    "image_source = Image.fromarray(color_img).convert(\"RGB\")\n",
    "image = np.asarray(image_source)\n",
    "final_mask_image = wrapper.segment(image, args.text, \n",
    "                                #    box_threshold=0.3, \n",
    "                                #    text_threshold=0.25, \n",
    "                                #    filter_threshold=50\n",
    "                                   )\n",
    "\n",
    "from scipy.ndimage import label, binary_erosion, binary_dilation\n",
    "final_mask_image_array = np.array(final_mask_image)\n",
    "separated_mask, _ = label(final_mask_image_array)\n",
    "\n",
    "\n",
    "final_mask_image = Image.fromarray(separated_mask).convert(\"L\")\n",
    "segmentaiton_annotation_path = os.path.join(annotation_path, \"frame_annotation.png\")\n",
    "final_mask_image.putpalette(get_palette())\n",
    "final_mask_image.save(segmentaiton_annotation_path)\n",
    "print(f\"Save to {segmentaiton_annotation_path}\")\n",
    "plt.imshow(np.array(final_mask_image))\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vos_3d_real",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
