{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[33m[robosuite WARNING] \u001b[0mNo private macro file found! (__init__.py:7)\n",
      "\u001b[1m\u001b[33m[robosuite WARNING] \u001b[0mIt is recommended to use a private macro file (__init__.py:8)\n",
      "\u001b[1m\u001b[33m[robosuite WARNING] \u001b[0mTo setup, run: python /home/yifengz/miniconda3/envs/dinov2/lib/python3.9/site-packages/robosuite/scripts/setup_macros.py (__init__.py:9)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jupyter environment detected. Enabling Open3D WebVisualizer.\n",
      "[Open3D INFO] WebRTC GUI backend enabled.\n",
      "[Open3D INFO] WebRTCWindowSystem: HTTP handshake server disabled.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pybullet build time: May 20 2022 19:45:31\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION\"] = \"python\"\n",
    "\n",
    "import json\n",
    "import yaml\n",
    "from orion.algos.human_video_oogs import HumanVideoOOGs\n",
    "from orion.algos.oog import OpenWorldObjectSceneGraph\n",
    "from orion.utils.misc_utils import *\n",
    "from orion.utils.o3d_utils import *\n",
    "from orion.utils.correspondence_utils import CorrespondenceModel\n",
    "from orion.utils.real_robot_utils import ImageCapturer\n",
    "from orion.utils.log_utils import get_orion_logger\n",
    "\n",
    "from deoxys import config_root\n",
    "from deoxys.franka_interface import FrankaInterface\n",
    "from deoxys.utils import YamlConfig\n",
    "from deoxys.utils.input_utils import input2action\n",
    "from deoxys.utils.io_devices import SpaceMouse\n",
    "from deoxys.utils.log_utils import get_deoxys_example_logger\n",
    "\n",
    "from deoxys_vision.networking.camera_redis_interface import \\\n",
    "    CameraRedisSubInterface\n",
    "from deoxys_vision.utils.calibration_utils import load_default_extrinsics, load_default_intrinsics\n",
    "from deoxys_vision.utils.camera_utils import assert_camera_ref_convention, get_camera_info\n",
    "from deoxys_vision.utils.img_utils import save_depth_in_rgb\n",
    "\n",
    "logger = get_orion_logger(__name__)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image capturing and model initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'output_dir': '', 'opts': [], 'pretrained_weights': 'https://dl.fbaipublicfiles.com/dinov2/dinov2_vitb14/dinov2_vitb14_pretrain.pth', 'config_file': '/home/yifengz/workspace/ORION/orion/../third_party/dinov2/dinov2/configs/eval/vitb14_pretrain.yaml'}\n"
     ]
    }
   ],
   "source": [
    "image_capturer = ImageCapturer()\n",
    "human_video_oogs = HumanVideoOOGs()\n",
    "\n",
    "# The correspondence model from GROOT to establish correspondences between segmentations, avoid running Grounded-SAM everytime. \n",
    "correspondence_model = CorrespondenceModel()\n",
    "human_video_oogs.set_correspondence_model(correspondence_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Real robot experiment parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deviation_angle_threshold = 30\n",
    "skip_trivial_solution = True\n",
    "high_occlusion = False\n",
    "offset = [0.0, -0.02, 0.03]\n",
    "velocity_threshold = 1.0\n",
    "target_dist_threshold = 0.01\n",
    "target_intersection_threshold = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "runtime_folder, rollout_folder, human_video_annotation_path = read_from_runtime_file()\n",
    "\n",
    "robot_video_annotation_path = rollout_folder\n",
    "\n",
    "human_video_oogs.generate_from_human_video(human_video_annotation_path)\n",
    "human_video_oogs.plan_inference(velocity_threshold=velocity_threshold,\n",
    "                                target_dist_threshold=target_dist_threshold, \n",
    "                                target_intersection_threshold=target_intersection_threshold)\n",
    "\n",
    "tmp_annotation_path = os.path.join(runtime_folder, \"tmp_annotation.png\")\n",
    "\n",
    "is_first_frame = True\n",
    "if os.path.exists(tmp_annotation_path):\n",
    "    is_first_frame = False\n",
    "\n",
    "robot_object_graph = OpenWorldObjectSceneGraph()\n",
    "if is_first_frame:\n",
    "\n",
    "    robot_depth_path = os.path.join(robot_video_annotation_path, \"depth.png\") # \"traj_exp/testing/assembly_plane_testing_2_depth.png\"\n",
    "    robot_first_frame, robot_annotation = get_first_frame_annotation(robot_video_annotation_path)\n",
    "\n",
    "    # previous implementation\n",
    "    # dataset_name = get_dataset_name_from_annotation(reference_path)\n",
    "    # info = load_reconstruction_info_from_human_demo(dataset_name)\n",
    "\n",
    "    last_obs = image_capturer.get_last_obs()\n",
    "\n",
    "    aff = robot_object_graph.generate_from_robot_demo(\n",
    "        input_image=robot_first_frame,\n",
    "        input_depth=load_depth_in_rgb(robot_depth_path),\n",
    "        camera_intrinsics=last_obs[\"intrinsics\"],\n",
    "        camera_extrinsics=last_obs[\"extrinsics\"],\n",
    "        # Comment this line, and the model will first use SAM to get a glboal segmentation mask\n",
    "        input_annotation=robot_annotation,\n",
    "        reference_graph=human_video_oogs.get_graph(0),\n",
    "        is_first_frame=is_first_frame,\n",
    "        correspondence_model=human_video_oogs.correspondence_model,\n",
    "    )\n",
    "    update_annotation = Image.fromarray(robot_object_graph.input_annotation)\n",
    "    update_annotation.putpalette(get_palette())\n",
    "    update_annotation.save(os.path.join(robot_video_annotation_path, \"frame_annotation.png\"))\n",
    "\n",
    "else:\n",
    "    logger.info(\"loading from previous step\")\n",
    "    last_obs = image_capturer.get_last_obs()\n",
    "    current_image, current_depth = last_obs[\"color_img\"], last_obs[\"depth_img\"]\n",
    "    # robot_depth_path = os.path.join(robot_video_annotation_path, \"depth.png\") # \"traj_exp/testing/assembly_plane_testing_2_depth.png\"\n",
    "\n",
    "    tmp_image_path = os.path.join(runtime_folder, \"tmp.jpg\")\n",
    "\n",
    "    robot_annotation = np.array(Image.open(tmp_annotation_path))\n",
    "    # dataset_name = get_dataset_name_from_annotation(reference_path)\n",
    "    # info = load_reconstruction_info_from_human_demo(dataset_name)\n",
    "\n",
    "    aff = robot_object_graph.generate_from_robot_demo(\n",
    "        input_image=current_image[..., ::-1],\n",
    "        input_depth=current_depth,\n",
    "        camera_intrinsics=last_obs[\"intrinsics\"],\n",
    "        camera_extrinsics=last_obs[\"extrinsics\"],\n",
    "        # Comment this line, and the model will first use SAM to get a glboal segmentation mask\n",
    "        input_annotation=robot_annotation,\n",
    "        reference_graph=human_video_oogs.get_graph(0),\n",
    "        is_first_frame=is_first_frame,\n",
    "        correspondence_model=human_video_oogs.correspondence_model,\n",
    "    )\n",
    "\n",
    "# human_video_oogs.get_graph(0).draw_overlay_image(mode=\"all\")\n",
    "robot_object_graph.draw_overlay_image(mode=\"object\")\n",
    "# human_video_oogs.get_graph(0).draw_dense_correspondence(robot_object_graph, object_ids=[2])\n",
    "\n",
    "robot_object_graph.compute_contact_states(\n",
    "    # dist_threshold=0.005, \n",
    "    # intersection_threshold=50\n",
    "    )\n",
    "# matched_graph_idx = 3\n",
    "print(human_video_oogs.get_oog_mode_sequence())\n",
    "matched_graph_idx = human_video_oogs.find_matching_oog_idx(robot_object_graph)\n",
    "current_oog_idx = matched_graph_idx\n",
    "\n",
    "manipulate_object_id = human_video_oogs.get_manipulate_object_seq()[matched_graph_idx]\n",
    "reference_object_id = human_video_oogs.get_reference_object_seq()[matched_graph_idx]\n",
    "logger.debug(f\"manipulate id: {manipulate_object_id}\")\n",
    "logger.debug(\"reference id: {reference_object_id}\")\n",
    "\n",
    "logger.info(f\"Mathced graph idx is: {matched_graph_idx}\")\n",
    "logger.debug(f\"Robot current contact state is: {robot_object_graph.contact_states}\")\n",
    "\n",
    "if matched_graph_idx >= human_video_oogs.num_graphs - 1:\n",
    "    logger.info(\"Finished!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subgoal_transform, target_transform = human_video_oogs.compute_subgoal(\n",
    "    matched_idx=matched_graph_idx,\n",
    "    robot_object_graph=robot_object_graph,\n",
    "    manipulate_object_id=manipulate_object_id,\n",
    "    reference_object_id=reference_object_id,\n",
    "    high_occlusion=high_occlusion,\n",
    "    deviation_angle_threshold=deviation_angle_threshold,\n",
    "    skip_trivial_solution=skip_trivial_solution\n",
    ")\n",
    "\n",
    "R_seq, t_seq,  best_loss, training_data = robot_object_graph.estimate_motion_traj_from_object(\n",
    "    object_id=manipulate_object_id, \n",
    "    use_visibility=True,\n",
    "    skip_interval=1,\n",
    "    # select_subset=3,\n",
    "    mode=\"lie\",\n",
    "    regularization_weight_pos=0.1,\n",
    "    regularization_weight_rot=0.1,    \n",
    "    num_max_iter=3,\n",
    "    high_occlusion=high_occlusion,\n",
    "    optim_kwargs={\n",
    "        \"lr\": 0.01,\n",
    "        \"num_epochs\": 501,\n",
    "        \"verbose\": True,\n",
    "        \"momentum\": 0.9,\n",
    "    }\n",
    ")\n",
    "\n",
    "logger.debug(\"target transforma: \", target_transform)\n",
    "\n",
    "object_1_pcd_points, object_1_pcd_colors = robot_object_graph.get_objects_3d_points(object_id=manipulate_object_id, remove_outlier_kwargs={\"nb_neighbors\": 30, \"std_ratio\": 0.7})\n",
    "object_2_pcd_points, object_2_pcd_colors = robot_object_graph.get_objects_3d_points(object_id=reference_object_id)\n",
    "demo_object_1_pcd_points, demo_object_1_pcd_colors = human_video_oogs.get_graph(current_oog_idx).get_objects_3d_points(object_id=manipulate_object_id)\n",
    "demo_object_2_pcd_points, demo_object_2_pcd_colors = human_video_oogs.get_graph(current_oog_idx).get_objects_3d_points(object_id=reference_object_id)\n",
    "\n",
    "print(np.sum(t_seq, axis=0))\n",
    "\n",
    "new_R = np.eye(3)\n",
    "new_R_seq = []\n",
    "R_seq = R_seq\n",
    "for R in R_seq:\n",
    "    new_R = R @ new_R\n",
    "    new_R_seq.append(new_R)\n",
    "print(new_R_seq[-1])\n",
    "\n",
    "new_points = []\n",
    "print(t_seq.shape)\n",
    "new_point = object_1_pcd_points\n",
    "for R, t in zip(R_seq, t_seq):\n",
    "    new_point = (R @ (new_point - new_point.mean(axis=0)).T ).T + new_point.mean(axis=0) + t[None, :]\n",
    "    # new_point = new_point + t[None, :]\n",
    "    new_points.append(new_point)\n",
    "\n",
    "def transform_point_clouds(transformation, points):\n",
    "    new_points = transformation @ np.concatenate((points, np.ones((points.shape[0], 1))), axis=1).T\n",
    "    new_points = new_points[:3, :].T\n",
    "    return new_points\n",
    "\n",
    "subgoal_points = human_video_oogs.get_graph(current_oog_idx).get_world_trajs(object_ids=[manipulate_object_id])[:, -1, :]\n",
    "subgoal_points = transform_point_clouds(subgoal_transform, subgoal_points)\n",
    "\n",
    "object_1_new_points = new_points[-1]\n",
    "\n",
    "manipulation_offset = object_1_pcd_points.mean(axis=0) - demo_object_1_pcd_points.mean(axis=0)\n",
    "reference_offset = object_2_pcd_points.mean(axis=0) - demo_object_2_pcd_points.mean(axis=0)\n",
    "new_demo_object_1_pcd_points = transform_point_clouds(target_transform, demo_object_1_pcd_points + manipulation_offset)\n",
    "\n",
    "eef_node = human_video_oogs.get_graph(current_oog_idx).eef_node\n",
    "gripper_action = eef_node.get_eef_action(verbose=True)\n",
    "\n",
    "interaction_points = eef_node.interaction_affordance.get_interaction_points(include_centroid=False)\n",
    "interaction_points = transform_point_clouds(target_transform, interaction_points + manipulation_offset)\n",
    "\n",
    "interaction_offset = np.array(offset).reshape(1, 3)\n",
    "interaction_centroid = eef_node.interaction_affordance.get_affordance_centroid()\n",
    "interaction_centroid = transform_point_clouds(target_transform, interaction_centroid + manipulation_offset)\n",
    "interaction_centroid += interaction_offset\n",
    "\n",
    "\n",
    "z_rotation = np.arctan2(target_transform[1, 0], target_transform[0, 0])\n",
    "\n",
    "print(object_1_new_points.mean(axis=0), object_1_pcd_points.mean(axis=0))\n",
    "print(\"z rotation: \", z_rotation)\n",
    "\n",
    "new_demo_object_2_pcd_points = transform_point_clouds(subgoal_transform, demo_object_2_pcd_points + reference_offset)\n",
    "\n",
    "pcd = np.concatenate((\n",
    "    object_1_pcd_points,\n",
    "    object_1_new_points, \n",
    "    new_demo_object_1_pcd_points,\n",
    "    object_2_pcd_points,\n",
    "    new_demo_object_2_pcd_points,\n",
    "    ), axis=0)\n",
    "colors = np.concatenate((\n",
    "    object_1_pcd_colors, \n",
    "    object_1_pcd_colors[..., [1, 0, 2]], \n",
    "    demo_object_1_pcd_colors[..., [1, 2, 0]] / 2,\n",
    "    object_2_pcd_colors,\n",
    "    demo_object_2_pcd_colors[..., ::-1],\n",
    "    ), axis=0)\n",
    "\n",
    "plotly_fig = plotly_draw_3d_pcd(pcd, colors, addition_points=interaction_points, marker_size=15)\n",
    "# plotly_offline_visualization([plotly_fig], \"runtime_visualization.html\")\n",
    "\n",
    "runtime_T_file = os.path.join(robot_video_annotation_path, f\"T_{matched_graph_idx}_seq.pt\")\n",
    "torch.save({\"R_seq\": R_seq, \"t_seq\": t_seq, \n",
    "            \"target_object_centroid\": object_1_pcd_points.mean(axis=0),\n",
    "            \"target_interaction_centroid\": interaction_centroid,\n",
    "            \"target_interaction_points\": interaction_points,\n",
    "            \"z_rotation\": z_rotation\n",
    "            }, runtime_T_file)\n",
    "\n",
    "with open(\"experiments/runtime_T.json\", \"w\") as f:\n",
    "    json.dump({\"file\": runtime_T_file}, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from orion.algos.oog import OOGMode\n",
    "oog_mode_seq = human_video_oogs.get_oog_mode_sequence()\n",
    "current_idx = 0\n",
    "\n",
    "def skip_free_motion(oog_mode_seq, current_idx):\n",
    "    while oog_mode_seq[current_idx] == OOGMode.FREE_MOTION:\n",
    "        current_idx += 1\n",
    "    return current_idx\n",
    "\n",
    "\n",
    "\n",
    "current_idx = skip_free_motion(oog_mode_seq, current_idx)\n",
    "print(current_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"manipulate id: \", manipulate_object_id)\n",
    "print(\"reference id: \", reference_object_id)\n",
    "\n",
    "current_image, current_depth = capture_image()\n",
    "# robot_depth_path = os.path.join(robot_video_annotation_path, \"depth.png\") # \"traj_exp/testing/assembly_plane_testing_2_depth.png\"\n",
    "\n",
    "tmp_image_path = os.path.join(runtime_folder, \"tmp.jpg\")\n",
    "tmp_annotation_path = os.path.join(runtime_folder, \"tmp_annotation.png\")\n",
    "\n",
    "# current_image = cv2.imread(tmp_image_path)[..., ::-1]\n",
    "\n",
    "# _, robot_annotation = get_first_frame_annotation(robot_video_annotation_path)\n",
    "robot_annotation = np.array(Image.open(tmp_annotation_path))\n",
    "dataset_name = get_dataset_name_from_annotation(reference_path)\n",
    "info = load_reconstruction_info_from_human_demo(dataset_name)\n",
    "\n",
    "robot_object_graph = OpenWorldObjectSceneGraph()\n",
    "aff = robot_object_graph.generate_from_robot_demo(\n",
    "    input_image=current_image[..., ::-1],\n",
    "    input_depth=current_depth,\n",
    "    camera_intrinsics=info[\"intrinsics\"],\n",
    "    camera_extrinsics=info[\"extrinsics\"],\n",
    "    # Comment this line, and the model will first use SAM to get a glboal segmentation mask\n",
    "    input_annotation=robot_annotation,\n",
    "    reference_graph=human_video_oogs.get_graph(0),\n",
    "    is_first_frame=True,\n",
    "    correspondence_model=human_video_oogs.correspondence_model,\n",
    ")\n",
    "\n",
    "plotly_draw_image(load_depth_in_rgb(robot_depth_path))\n",
    "robot_object_graph.compute_contact_states()\n",
    "print(robot_object_graph.contact_states)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dinov2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
